{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking Methods in RAG Systems\n",
    "\n",
    "## Overview\n",
    "Reranking is a crucial step in Retrieval-Augmented Generation (RAG) systems that aims to improve the relevance and quality of retrieved documents. It involves reassessing and reordering initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing or presentation.\n",
    "\n",
    "## Motivation\n",
    "The primary motivation for reranking in RAG systems is to overcome limitations of initial retrieval methods, which often rely on simpler similarity metrics. Reranking allows for more sophisticated relevance assessment, taking into account nuanced relationships between queries and documents that might be missed by traditional retrieval techniques. This process aims to enhance the overall performance of RAG systems by ensuring that the most relevant information is used in the generation phase.\n",
    "\n",
    "## Key Components\n",
    "Reranking systems typically include the following components:\n",
    "\n",
    "1. Initial Retriever: Often a vector store using embedding-based similarity search.\n",
    "2. Reranking Model: This can be either:\n",
    "   - A Large Language Model (LLM) for scoring relevance\n",
    "   - A Cross-Encoder model specifically trained for relevance assessment\n",
    "3. Scoring Mechanism: A method to assign relevance scores to documents\n",
    "4. Sorting and Selection Logic: To reorder documents based on new scores\n",
    "\n",
    "## Method Details\n",
    "The reranking process generally follows these steps:\n",
    "\n",
    "1. Initial Retrieval: Fetch an initial set of potentially relevant documents.\n",
    "2. Pair Creation: Form query-document pairs for each retrieved document.\n",
    "3. Scoring: \n",
    "   - LLM Method: Use prompts to ask the LLM to rate document relevance.\n",
    "   - Cross-Encoder Method: Feed query-document pairs directly into the model.\n",
    "4. Score Interpretation: Parse and normalize the relevance scores.\n",
    "5. Reordering: Sort documents based on their new relevance scores.\n",
    "6. Selection: Choose the top K documents from the reordered list.\n",
    "\n",
    "## Benefits of this Approach\n",
    "Reranking offers several advantages:\n",
    "\n",
    "1. Improved Relevance: By using more sophisticated models, reranking can capture subtle relevance factors.\n",
    "2. Flexibility: Different reranking methods can be applied based on specific needs and resources.\n",
    "3. Enhanced Context Quality: Providing more relevant documents to the RAG system improves the quality of generated responses.\n",
    "4. Reduced Noise: Reranking helps filter out less relevant information, focusing on the most pertinent content.\n",
    "\n",
    "## Conclusion\n",
    "Reranking is a powerful technique in RAG systems that significantly enhances the quality of retrieved information. Whether using LLM-based scoring or specialized Cross-Encoder models, reranking allows for more nuanced and accurate assessment of document relevance. This improved relevance translates directly to better performance in downstream tasks, making reranking an essential component in advanced RAG implementations.\n",
    "\n",
    "The choice between LLM-based and Cross-Encoder reranking methods depends on factors such as required accuracy, available computational resources, and specific application needs. Both approaches offer substantial improvements over basic retrieval methods and contribute to the overall effectiveness of RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!uv pip install langchain langchain-openai langchain-community langchain-text-splitters python-dotenv sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.docstore.document import Document\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "from utils.helper_functions import *\n",
    "from utils.evaluate_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the document's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = encode_pdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x139da86e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: LLM based function to rerank the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom reranking function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingScore(BaseModel):\n",
    "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")\n",
    "\n",
    "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"doc\"],\n",
    "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
    "    \n",
    "    scored_docs = []\n",
    "    for doc in docs:\n",
    "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
    "        score = llm_chain.invoke(input_data).relevance_score\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0  # Default score if parsing fails\n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage of the reranking function with a sample query relevant to the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top initial documents:\n",
      "\n",
      "Document 1:\n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
      "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
      "experiencing shi...\n",
      "\n",
      "Document 2:\n",
      "goals. Policies should promote synergies between biodiversity conservation and climate \n",
      "action. \n",
      "Chapter 10: Climate Change and Human Health \n",
      "Health Impacts \n",
      "Heat-Related Illnesses \n",
      "Rising temperature...\n",
      "\n",
      "Document 3:\n",
      "managed retreats. \n",
      "Extreme Weather Events \n",
      "Climate change is linked to an increase in the frequency and severity of extreme weather \n",
      "events, such as hurricanes, heatwaves, droughts, and heavy rainfall...\n",
      "Query: What are the impacts of climate change on biodiversity?\n",
      "\n",
      "Top reranked documents:\n",
      "\n",
      "Document 1:\n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
      "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
      "experiencing shi...\n",
      "\n",
      "Document 2:\n",
      "protection, and habitat creation. \n",
      "Climate-Resilient Conservation \n",
      "Conservation strategies must account for climate change impacts to be effective. This \n",
      "includes identifying climate refugia, areas le...\n",
      "\n",
      "Document 3:\n",
      "Coral reefs are highly sensitive to changes in temperature and acidity. Ocean acidification \n",
      "and warming waters contribute to coral bleaching and mortality, threatening biodiversity and \n",
      "fisheries. Pr...\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "initial_docs = vectorstore.similarity_search(query, k=15)\n",
    "reranked_docs = rerank_documents(query, initial_docs)\n",
    "\n",
    "# print first 3 initial documents\n",
    "print(\"Top initial documents:\")\n",
    "for i, doc in enumerate(initial_docs[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top reranked documents:\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom retriever based on our reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom retriever class\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    \n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
    "        return rerank_documents(query, initial_docs, top_n=num_docs)\n",
    "    \n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self._get_relevant_documents(query)\n",
    "\n",
    "\n",
    "# Create the custom retriever\n",
    "custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "\n",
    "# Create an LLM for answering questions\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# Create the RetrievalQA chain with the custom retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=custom_retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mp/b24cjdwd53j8wrh3z14b59qw0000gn/T/ipykernel_22164/546735038.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use `invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the impacts of climate change on biodiversity?\n",
      "Answer: Climate change impacts biodiversity by altering terrestrial and marine ecosystems. It shifts habitat ranges, changes species distributions, and affects ecosystem functions. In terrestrial ecosystems, forests, grasslands, and deserts experience changes in plant and animal species composition, which can lead to a loss of biodiversity and disrupt ecological balance. In marine ecosystems, rising sea temperatures, ocean acidification, and changing currents affect marine biodiversity, including coral reefs and deep-sea habitats. These changes can result in species migration, altered reproductive cycles, and disruptions in marine food webs and fisheries. Overall, climate change poses significant threats to biodiversity across various ecosystems.\n",
      "\n",
      "Relevant source documents:\n",
      "\n",
      "Document 1:\n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
      "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
      "experiencing shi...\n",
      "\n",
      "Document 2:\n",
      "protection, and habitat creation. \n",
      "Climate-Resilient Conservation \n",
      "Conservation strategies must account for climate change impacts to be effective. This \n",
      "includes identifying climate refugia, areas le...\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example that demonstrates why we should use reranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Retrieval Techniques\n",
      "==================================\n",
      "Query: what is the capital of france?\n",
      "\n",
      "Baseline Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is beautiful.\n",
      "\n",
      "Advanced Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is beautiful.\n"
     ]
    }
   ],
   "source": [
    "chunks = [\n",
    "    \"The capital of France is great.\",\n",
    "    \"The capital of France is huge.\",\n",
    "    \"The capital of France is beautiful.\",\n",
    "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
    "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\", \n",
    "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
    "]\n",
    "docs = [Document(page_content=sentence) for sentence in chunks]\n",
    "\n",
    "\n",
    "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    print(\"Comparison of Retrieval Techniques\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    print(\"Baseline Retrieval Result:\")\n",
    "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
    "    for i, doc in enumerate(baseline_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "    print(\"\\nAdvanced Retrieval Result:\")\n",
    "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "    advanced_docs = custom_retriever.invoke(query)\n",
    "    for i, doc in enumerate(advanced_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "\n",
    "query = \"what is the capital of france?\"\n",
    "compare_rag_techniques(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Cross Encoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cross encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8fee792d4d4deda06012f93f2800d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3a72217e9f4a5995384c5a2884eca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2d2fdd330c45dcab8a7b289acfe74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff248923718b451caacf8a0de34f51bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03df875795714d9eb2659d0c1c2b2938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a1c0f83f7b47b8aa4e63362ee2e7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62cc134456a4fae86830001010c516c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic import ConfigDict\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "    cross_encoder: Any = Field(description=\"Cross-encoder model for reranking\")\n",
    "    k: int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
    "    rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Initial retrieval\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "        \n",
    "        # Prepare pairs for cross-encoder\n",
    "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Sort documents by score\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top reranked documents\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self._get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance and showcase over an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the impacts of climate change on biodiversity?\n",
      "Answer: Climate change impacts biodiversity by altering terrestrial and marine ecosystems. In terrestrial ecosystems, it shifts habitat ranges, changes species distributions, and impacts ecosystem functions, leading to a loss of biodiversity and disrupting ecological balance. In marine ecosystems, rising sea temperatures, ocean acidification, and changing currents affect marine biodiversity, disrupting marine food webs and fisheries. These changes can lead to species migration and altered reproductive cycles, further impacting biodiversity.\n",
      "\n",
      "Relevant source documents:\n",
      "\n",
      "Document 1:\n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
      "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
      "experiencing shi...\n",
      "\n",
      "Document 2:\n",
      "protection, and habitat creation. \n",
      "Climate-Resilient Conservation \n",
      "Conservation strategies must account for climate change impacts to be effective. This \n",
      "includes identifying climate refugia, areas le...\n",
      "\n",
      "Document 3:\n",
      "goals. Policies should promote synergies between biodiversity conservation and climate \n",
      "action. \n",
      "Chapter 10: Climate Change and Human Health \n",
      "Health Impacts \n",
      "Heat-Related Illnesses \n",
      "Rising temperature...\n",
      "\n",
      "Document 4:\n",
      "Local communities are often on the front lines of climate impacts and can be powerful agents \n",
      "of change. Community-based conservation projects involve residents in protecting and \n",
      "restoring natural re...\n",
      "\n",
      "Document 5:\n",
      "cultural perceptions. \n",
      "Youth Engagement \n",
      "Youth are vital stakeholders in climate action. Empowering young people through education, \n",
      "activism, and leadership opportunities can drive transformative cha...\n"
     ]
    }
   ],
   "source": [
    "# Create the cross-encoder retriever\n",
    "cross_encoder_retriever = CrossEncoderRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    cross_encoder=cross_encoder,\n",
    "    k=10,  # Retrieve 10 documents initially\n",
    "    rerank_top_k=5  # Return top 5 after reranking\n",
    ")\n",
    "\n",
    "# Set up the LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "# Create the RetrievalQA chain with the cross-encoder retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=cross_encoder_retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the main causes of climate change?', 'result': 'The main causes of climate change include:\\n\\n1. **Increase in Greenhouse Gases**: The primary cause of recent climate change is the rise in greenhouse gases in the atmosphere, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O). These gases trap heat from the sun, intensifying the greenhouse effect.\\n\\n2. **Burning of Fossil Fuels**: The consumption of fossil fuels (coal, oil, and natural gas) for energy releases large amounts of CO2 into the atmosphere. This began significantly with the industrial revolution and continues to rise today.\\n\\n3. **Deforestation**: The clearing of forests for agriculture, development, and other purposes reduces the number of trees that can absorb CO2, contributing to higher atmospheric carbon levels.\\n\\nThese human activities have greatly intensified natural processes that lead to climate change.'}\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Using Cohere Re-ranker\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "#from langchain_classic.retrievers.document_compressors import CohereRerank\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(path   )\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "# Create vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Setup re-ranker\n",
    "#compressor = CohereRerank(model=\"rerank-english-v3.0\", top_n=3)\n",
    "model = HuggingFaceCrossEncoder(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Create QA chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever\n",
    ")\n",
    "\n",
    "# Query\n",
    "response = qa_chain.invoke(\"What are the main causes of climate change?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 23:17:48,426 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-17 23:17:49,633 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-17 23:17:50,038 - INFO - HTTP Request: POST https://api.cohere.com/v1/rerank \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "UnauthorizedError",
     "evalue": "headers: {'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'date': 'Sat, 17 Jan 2026 19:17:50 GMT', 'x-envoy-upstream-service-time': '2', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 401, body: {'id': '0a4fa52f-f8cf-4289-8db3-fffcc9a16075', 'message': 'invalid api token'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnauthorizedError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Query with re-ranking\u001b[39;00m\n\u001b[32m     24\u001b[39m query_engine = index.as_query_engine(\n\u001b[32m     25\u001b[39m     similarity_top_k=\u001b[32m10\u001b[39m,\n\u001b[32m     26\u001b[39m     node_postprocessors=[rerank]\n\u001b[32m     27\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m response = \u001b[43mquery_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat are the main causes of climate change?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index/core/base/base_query_engine.py:44\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     43\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m dispatcher.event(\n\u001b[32m     46\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     47\u001b[39m )\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index/core/query_engine/retriever_query_engine.py:196\u001b[39m, in \u001b[36mRetrieverQueryEngine._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    194\u001b[39m     CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001b[32m    195\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._response_synthesizer.synthesize(\n\u001b[32m    198\u001b[39m         query=query_bundle,\n\u001b[32m    199\u001b[39m         nodes=nodes,\n\u001b[32m    200\u001b[39m     )\n\u001b[32m    201\u001b[39m     query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index/core/query_engine/retriever_query_engine.py:150\u001b[39m, in \u001b[36mRetrieverQueryEngine.retrieve\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) -> List[NodeWithScore]:\n\u001b[32m    149\u001b[39m     nodes = \u001b[38;5;28mself\u001b[39m._retriever.retrieve(query_bundle)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_node_postprocessors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index/core/query_engine/retriever_query_engine.py:134\u001b[39m, in \u001b[36mRetrieverQueryEngine._apply_node_postprocessors\u001b[39m\u001b[34m(self, nodes, query_bundle)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply_node_postprocessors\u001b[39m(\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m, nodes: List[NodeWithScore], query_bundle: QueryBundle\n\u001b[32m    132\u001b[39m ) -> List[NodeWithScore]:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node_postprocessor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._node_postprocessors:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         nodes = \u001b[43mnode_postprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpostprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index/core/postprocessor/types.py:48\u001b[39m, in \u001b[36mBaseNodePostprocessor.postprocess_nodes\u001b[39m\u001b[34m(self, nodes, query_bundle, query_str)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_postprocess_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/llama_index/postprocessor/cohere_rerank/base.py:81\u001b[39m, in \u001b[36mCohereRerank._postprocess_nodes\u001b[39m\u001b[34m(self, nodes, query_bundle)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m     69\u001b[39m     CBEventType.RERANKING,\n\u001b[32m     70\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m     },\n\u001b[32m     76\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m     77\u001b[39m     texts = [\n\u001b[32m     78\u001b[39m         node.node.get_content(metadata_mode=MetadataMode.EMBED)\n\u001b[32m     79\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[32m     80\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     new_nodes = []\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results.results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/cohere/base_client.py:1211\u001b[39m, in \u001b[36mBaseCohere.rerank\u001b[39m\u001b[34m(self, query, documents, model, top_n, rank_fields, return_documents, max_chunks_per_doc, request_options)\u001b[39m\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrerank\u001b[39m(\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1130\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1138\u001b[39m     request_options: typing.Optional[RequestOptions] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1139\u001b[39m ) -> RerankResponse:\n\u001b[32m   1140\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[33;03m    This endpoint takes in a query and a list of texts and produces an ordered array with each text assigned a relevance score.\u001b[39;00m\n\u001b[32m   1142\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1209\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m   1210\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m     _response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrank_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrank_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_documents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_chunks_per_doc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_chunks_per_doc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workstation/Codes/AI-Workspace/Advance-RAG/.venv/lib/python3.13/site-packages/cohere/raw_base_client.py:1899\u001b[39m, in \u001b[36mRawBaseCohere.rerank\u001b[39m\u001b[34m(self, query, documents, model, top_n, rank_fields, return_documents, max_chunks_per_doc, request_options)\u001b[39m\n\u001b[32m   1888\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[32m   1889\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m   1890\u001b[39m         body=typing.cast(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1896\u001b[39m         ),\n\u001b[32m   1897\u001b[39m     )\n\u001b[32m   1898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _response.status_code == \u001b[32m401\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1899\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnauthorizedError(\n\u001b[32m   1900\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m   1901\u001b[39m         body=typing.cast(\n\u001b[32m   1902\u001b[39m             typing.Optional[typing.Any],\n\u001b[32m   1903\u001b[39m             construct_type(\n\u001b[32m   1904\u001b[39m                 type_=typing.Optional[typing.Any],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1905\u001b[39m                 object_=_response.json(),\n\u001b[32m   1906\u001b[39m             ),\n\u001b[32m   1907\u001b[39m         ),\n\u001b[32m   1908\u001b[39m     )\n\u001b[32m   1909\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m   1910\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ForbiddenError(\n\u001b[32m   1911\u001b[39m         headers=\u001b[38;5;28mdict\u001b[39m(_response.headers),\n\u001b[32m   1912\u001b[39m         body=typing.cast(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1918\u001b[39m         ),\n\u001b[32m   1919\u001b[39m     )\n",
      "\u001b[31mUnauthorizedError\u001b[39m: headers: {'cache-control': 'no-cache, no-store, no-transform, must-revalidate, private, max-age=0', 'content-encoding': 'gzip', 'content-type': 'application/json', 'expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'pragma': 'no-cache', 'vary': 'Origin,Accept-Encoding', 'x-accel-expires': '0', 'date': 'Sat, 17 Jan 2026 19:17:50 GMT', 'x-envoy-upstream-service-time': '2', 'server': 'envoy', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'transfer-encoding': 'chunked'}, status_code: 401, body: {'id': '0a4fa52f-f8cf-4289-8db3-fffcc9a16075', 'message': 'invalid api token'}"
     ]
    }
   ],
   "source": [
    "# Option 1: Using Cohere Re-ranker\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "#from llama_index.postprocessor.sentence_transformer_rerank import SentenceTransformerRerank\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Configure settings\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "# Load PDF\n",
    "documents = SimpleDirectoryReader(input_files=[path]).load_data()\n",
    "\n",
    "# Create index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Setup re-ranker\n",
    "rerank = CohereRerank(api_key=\"COHERE_API_KEY\", top_n=3)\n",
    "\n",
    "\n",
    "# Query with re-ranking\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[rerank]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the main causes of climate change?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 23:28:03,021 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-17 23:28:04,065 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-17 23:28:06,399 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-01-17 23:28:07,936 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main causes of climate change are the increase in greenhouse gases in the atmosphere, primarily carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), which trap heat from the sun and create a \"greenhouse effect.\" Human activities, such as burning fossil fuels like coal, oil, and natural gas, as well as deforestation, have intensified this natural process, leading to a warmer climate.\n"
     ]
    }
   ],
   "source": [
    "# Option 3: Using LLM Re-ranker (OpenAI-based)\n",
    "\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Load PDF\n",
    "documents = SimpleDirectoryReader(input_files=[path]).load_data()\n",
    "\n",
    "# Create index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "rerank = LLMRerank(top_n=3, llm=OpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[rerank]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What are the main causes of climate change?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
