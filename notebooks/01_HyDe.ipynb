{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embedding (HyDE) in Document Retrieval\n",
    "\n",
    "## Overview\n",
    "\n",
    "This code implements a Hypothetical Document Embedding (HyDE) system for document retrieval. HyDE is an innovative approach that transforms query questions into hypothetical documents containing the answer, aiming to bridge the gap between query and document distributions in vector space.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Traditional retrieval methods often struggle with the semantic gap between short queries and longer, more detailed documents. HyDE addresses this by expanding the query into a full hypothetical document, potentially improving retrieval relevance by making the query representation more similar to the document representations in the vector space.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. PDF processing and text chunking\n",
    "2. Vector store creation using FAISS and OpenAI embeddings\n",
    "3. Language model for generating hypothetical documents\n",
    "4. Custom HyDERetriever class implementing the HyDE technique\n",
    "\n",
    "## Method Details\n",
    "\n",
    "### Document Preprocessing and Vector Store Creation\n",
    "\n",
    "1. The PDF is processed and split into chunks.\n",
    "2. A FAISS vector store is created using OpenAI embeddings for efficient similarity search.\n",
    "\n",
    "### Hypothetical Document Generation\n",
    "\n",
    "1. A language model (GPT-4) is used to generate a hypothetical document that answers the given query.\n",
    "2. The generation is guided by a prompt template that ensures the hypothetical document is detailed and matches the chunk size used in the vector store.\n",
    "\n",
    "### Retrieval Process\n",
    "\n",
    "The `HyDERetriever` class implements the following steps:\n",
    "\n",
    "1. Generate a hypothetical document from the query using the language model.\n",
    "2. Use the hypothetical document as the search query in the vector store.\n",
    "3. Retrieve the most similar documents to this hypothetical document.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. Query Expansion: Transforms short queries into detailed hypothetical documents.\n",
    "2. Flexible Configuration: Allows adjustment of chunk size, overlap, and number of retrieved documents.\n",
    "3. Integration with OpenAI Models: Uses GPT-4 for hypothetical document generation and OpenAI embeddings for vector representation.\n",
    "\n",
    "## Benefits of this Approach\n",
    "\n",
    "1. Improved Relevance: By expanding queries into full documents, HyDE can potentially capture more nuanced and relevant matches.\n",
    "2. Handling Complex Queries: Particularly useful for complex or multi-faceted queries that might be difficult to match directly.\n",
    "3. Adaptability: The hypothetical document generation can adapt to different types of queries and document domains.\n",
    "4. Potential for Better Context Understanding: The expanded query might better capture the context and intent behind the original question.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "1. Uses OpenAI's ChatGPT model for hypothetical document generation.\n",
    "2. Employs FAISS for efficient similarity search in the vector space.\n",
    "3. Allows for easy visualization of both the hypothetical document and retrieved results.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Hypothetical Document Embedding (HyDE) represents an innovative approach to document retrieval, addressing the semantic gap between queries and documents. By leveraging advanced language models to expand queries into hypothetical documents, HyDE has the potential to significantly improve retrieval relevance, especially for complex or nuanced queries. This technique could be particularly valuable in domains where understanding query intent and context is crucial, such as legal research, academic literature review, or advanced information retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Installation and Imports\n",
    "\n",
    "The cell below installs all necessary packages required to run this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "#!uv pip install langchain langchain-openai langchain-community langchain-text-splitters python-dotenv pymupdf faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "from utils.helper_functions import *\n",
    "from utils.evaluate_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define document(s) path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the HyDe retriever class - creating vector store, generating hypothetical document, and retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyDERetriever:\n",
    "    def __init__(self, files_path, chunk_size=500, chunk_overlap=100):\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.vectorstore = encode_pdf(files_path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
    "    \n",
    "        \n",
    "        self.hyde_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"chunk_size\"],\n",
    "            template=\"\"\"Given the question '{query}', generate a hypothetical document that directly answers this question. The document should be detailed and in-depth.\n",
    "            the document size has be exactly {chunk_size} characters.\"\"\",\n",
    "        )\n",
    "        self.hyde_chain = self.hyde_prompt | self.llm\n",
    "\n",
    "    def generate_hypothetical_document(self, query):\n",
    "        input_variables = {\"query\": query, \"chunk_size\": self.chunk_size}\n",
    "        return self.hyde_chain.invoke(input_variables).content\n",
    "\n",
    "    def retrieve(self, query, k=3):\n",
    "        hypothetical_doc = self.generate_hypothetical_document(query)\n",
    "        similar_docs = self.vectorstore.similarity_search(hypothetical_doc, k=k)\n",
    "        return similar_docs, hypothetical_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a HyDe retriever instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = HyDERetriever(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate on a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What is the main cause of climate change?\"\n",
    "results, hypothetical_doc = retriever.retrieve(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the hypothetical document and the retrieved documnets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_content = [doc.page_content for doc in results]\n",
    "\n",
    "print(\"hypothetical_doc:\\n\")\n",
    "print(text_wrap(hypothetical_doc)+\"\\n\")\n",
    "show_context(docs_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain HyDe Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_classic.chains import HypotheticalDocumentEmbedder\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"data/Understanding_Climate_Change.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "# Create HYDE embeddings\n",
    "base_embeddings = OpenAIEmbeddings()\n",
    "llm = OpenAI(temperature=0)\n",
    "hyde_embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
    "    llm, base_embeddings, prompt_key=\"web_search\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(chunks, hyde_embeddings)\n",
    "\n",
    "# Query\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "docs = retriever.invoke(\"What are the roles of technology in Climate Change Mitigation?\")\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load PDF\n",
    "docs = PyPDFLoader(\"data/Understanding_Climate_Change.pdf\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000).split_documents(docs)\n",
    "\n",
    "# Create vector store and retriever\n",
    "retriever = Chroma.from_documents(chunks, OpenAIEmbeddings()).as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# HYDE prompt\n",
    "hyde_prompt = ChatPromptTemplate.from_template(\"Write a passage answering: {question}\")\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Create HYDE RAG chain - CORRECTED ‚úÖ\n",
    "hyde_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"hypothetical\": hyde_prompt | llm | StrOutputParser()}\n",
    "    | RunnableLambda(lambda x: {  # ‚úÖ Wrap lambda in RunnableLambda\n",
    "        \"context\": format_docs(retriever.invoke(x[\"hypothetical\"])), \n",
    "        \"question\": x[\"question\"]\n",
    "    })\n",
    "    | ChatPromptTemplate.from_template(\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Query\n",
    "answer = hyde_chain.invoke(\"What is this about?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"üìö Loading and processing PDF...\")\n",
    "docs = PyPDFLoader(\"data/Understanding_Climate_Change.pdf\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "print(f\"‚úÖ Created {len(chunks)} chunks from {len(docs)} pages\")\n",
    "\n",
    "print(\"\\nüíæ Creating vector store...\")\n",
    "vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "print(\"‚úÖ Vector store ready\")\n",
    "\n",
    "# Helper functions\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "def retrieve_direct(query):\n",
    "    \"\"\"Direct retrieval using the query.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return format_docs(docs)\n",
    "\n",
    "def retrieve_with_hyde(inputs):\n",
    "    \"\"\"Retrieve using hypothetical document.\"\"\"\n",
    "    hypothetical = inputs[\"hypothetical\"]\n",
    "    \n",
    "    # Show what was generated\n",
    "    print(f\"\\nüîÆ Generated hypothetical document:\")\n",
    "    print(f\"{hypothetical[:200]}...\\n\")\n",
    "    \n",
    "    docs = retriever.invoke(hypothetical)\n",
    "    \n",
    "    return {\n",
    "        \"context\": format_docs(docs),\n",
    "        \"question\": inputs[\"question\"]\n",
    "    }\n",
    "\n",
    "# QA Prompt\n",
    "qa_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: STANDARD RAG (Direct Retrieval)\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Building STANDARD RAG chain...\")\n",
    "standard_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(retrieve_direct),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"‚úÖ Standard RAG chain ready\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 2: HYDE RAG (Hypothetical Document Retrieval)\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Building HYDE RAG chain...\")\n",
    "hyde_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Write a detailed, informative passage that would answer this question. \n",
    "Include relevant facts, explanations, and context.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Passage:\"\"\"\n",
    ")\n",
    "\n",
    "hyde_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"hypothetical\": hyde_prompt | llm | StrOutputParser()\n",
    "    }\n",
    "    | RunnableLambda(retrieve_with_hyde)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"‚úÖ HYDE RAG chain ready\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARE RESULTS\n",
    "# ============================================================================\n",
    "query = \"What are the key findings about climate change?\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç STANDARD RAG (Direct Query Retrieval)\")\n",
    "print(\"=\" * 80)\n",
    "standard_answer = standard_chain.invoke(query)\n",
    "print(standard_answer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîÆ HYDE RAG (Hypothetical Document Retrieval)\")\n",
    "print(\"=\" * 80)\n",
    "hyde_answer = hyde_chain.invoke(query)\n",
    "print(hyde_answer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Standard answer length: {len(standard_answer)} chars\")\n",
    "print(f\"HYDE answer length: {len(hyde_answer)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install llama-index llama-index-llms-openai llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import os\n",
    "\n",
    "# Configure\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "# Set global settings\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "# Load PDF\n",
    "documents = SimpleDirectoryReader(input_files=[\"document.pdf\"]).load_data()\n",
    "\n",
    "# Create index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create HYDE query engine\n",
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    index.as_query_engine(),\n",
    "    query_transform=hyde\n",
    ")\n",
    "\n",
    "# Query\n",
    "response = hyde_query_engine.query(\"What is this document about?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "# Load PDF\n",
    "documents = SimpleDirectoryReader(input_files=[\"document.pdf\"]).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Standard query engine\n",
    "standard_engine = index.as_query_engine()\n",
    "\n",
    "# HYDE query engine\n",
    "hyde_engine = TransformQueryEngine(\n",
    "    index.as_query_engine(),\n",
    "    query_transform=HyDEQueryTransform(include_original=True)\n",
    ")\n",
    "\n",
    "# Compare\n",
    "query = \"What are the main conclusions?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STANDARD RETRIEVAL\")\n",
    "print(\"=\" * 80)\n",
    "print(standard_engine.query(query))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HYDE RETRIEVAL\")\n",
    "print(\"=\" * 80)\n",
    "print(hyde_engine.query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--hyde-hypothetical-document-embedding)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
